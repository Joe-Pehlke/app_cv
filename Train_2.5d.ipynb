{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is the original authors note, this notebook is actually based on another notebook that only provided\n",
    "#a inference notebook. \n",
    "\n",
    "#This notebook is found https://www.kaggle.com/code/royalacecat/training-nfl-2-5d-cnn-lb-0-671-with-tta\n",
    "\n",
    "#Unless otherwise suggested, all comments in cells are from Joe Pehlke "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T15:34:35.699592Z",
     "iopub.status.busy": "2023-01-27T15:34:35.699098Z",
     "iopub.status.idle": "2023-01-27T15:34:35.708483Z",
     "shell.execute_reply": "2023-01-27T15:34:35.706487Z",
     "shell.execute_reply.started": "2023-01-27T15:34:35.699558Z"
    }
   },
   "source": [
    "# If the training notebook is useful please upvote !!!\n",
    "\n",
    "### This notebook is based on the notebooks made by zzy.\n",
    "\n",
    "Please upvote the LB:0.667 original notebooks:\n",
    "\n",
    "https://www.kaggle.com/code/zzy990106/nfl-2-5d-cnn-baseline-inference\n",
    "\n",
    "also can use to LB:0.671, 2.5D CNN Baseline（More TTA trick）\n",
    "\n",
    "https://www.kaggle.com/code/royalacecat/lb-0-671-2-5d-cnn-baseline-more-tta-trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T22:07:28.196461Z",
     "iopub.status.busy": "2023-02-26T22:07:28.196152Z",
     "iopub.status.idle": "2023-02-26T22:07:33.298106Z",
     "shell.execute_reply": "2023-02-26T22:07:33.297065Z",
     "shell.execute_reply.started": "2023-02-26T22:07:28.196433Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "#Here we import cv2 only to load images, tried with pillow for a little bit, but gave in\n",
    "#as it was easy to just load with cv2.\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "from functools import lru_cache\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "#timm is a great library for pretrained models\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "#I attempted a step scheduler to see if there would be a speedup, but did not find much\n",
    "from timm.scheduler import StepLRScheduler\n",
    "sys.path.append('../input/timm-0-6-9/pytorch-image-models-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T22:07:33.300701Z",
     "iopub.status.busy": "2023-02-26T22:07:33.300325Z",
     "iopub.status.idle": "2023-02-26T22:07:33.308870Z",
     "shell.execute_reply": "2023-02-26T22:07:33.308041Z",
     "shell.execute_reply.started": "2023-02-26T22:07:33.300664Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here I changed the model to resnet18 and the learning rate to 1e-1, I noticed a large speedup when I made these\n",
    "#changes of approx 10 minutes an epoch\n",
    "CFG = {\n",
    "    'seed': 42,\n",
    "    'model': 'resnet18',\n",
    "    'img_size': 256,\n",
    "    'epochs': 10,\n",
    "    'train_bs': 8, \n",
    "    'valid_bs': 4,\n",
    "    'lr': 1e-1, \n",
    "    'weight_decay': 1e-6,\n",
    "    'num_workers': 20,\n",
    "    'max_grad_norm' : 1000,\n",
    "    'epochs_warmup' : 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T22:07:33.311011Z",
     "iopub.status.busy": "2023-02-26T22:07:33.310571Z",
     "iopub.status.idle": "2023-02-26T22:07:33.393522Z",
     "shell.execute_reply": "2023-02-26T22:07:33.390257Z",
     "shell.execute_reply.started": "2023-02-26T22:07:33.310958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['seed'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T22:07:33.396428Z",
     "iopub.status.busy": "2023-02-26T22:07:33.396153Z",
     "iopub.status.idle": "2023-02-26T22:07:33.402969Z",
     "shell.execute_reply": "2023-02-26T22:07:33.402030Z",
     "shell.execute_reply.started": "2023-02-26T22:07:33.396404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_contact_id(df):\n",
    "    \"\"\"\n",
    "    Splits out contact_id into seperate columns.\n",
    "    \"\"\"\n",
    "    df[\"game_play\"] = df[\"contact_id\"].str[:12]\n",
    "    df[\"step\"] = df[\"contact_id\"].str.split(\"_\").str[-3].astype(\"int\")\n",
    "    df[\"nfl_player_id_1\"] = df[\"contact_id\"].str.split(\"_\").str[-2]\n",
    "    df[\"nfl_player_id_2\"] = df[\"contact_id\"].str.split(\"_\").str[-1]\n",
    "    return df\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T22:07:33.405555Z",
     "iopub.status.busy": "2023-02-26T22:07:33.404422Z",
     "iopub.status.idle": "2023-02-26T22:08:28.582683Z",
     "shell.execute_reply": "2023-02-26T22:08:28.581731Z",
     "shell.execute_reply.started": "2023-02-26T22:07:33.405520Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = expand_contact_id(pd.read_csv(\"kaggle/train_labels.csv\"))\n",
    "train_tracking = pd.read_csv(\"kaggle/train_player_tracking.csv\")\n",
    "train_helmets = pd.read_csv(\"kaggle/train_baseline_helmets.csv\")\n",
    "train_video_metadata = pd.read_csv(\"kaggle/train_video_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T22:08:28.584604Z",
     "iopub.status.busy": "2023-02-26T22:08:28.584241Z"
    }
   },
   "outputs": [],
   "source": [
    "#!mkdir -p kaggle/train/frames\n",
    "\n",
    "#for video in tqdm(train_helmets.video.unique()):\n",
    "#    if 'Endzone2' not in video:\n",
    "#        !ffmpeg -i kaggle/train/{video} -q:v 2 -f image2 kaggle/train/frames/{video}_%04d.jpg -hide_banner -loglevel error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, tr_tracking, merge_col=\"step\", use_cols=[\"x_position\", \"y_position\"]):\n",
    "    output_cols = []\n",
    "    df_combo = (\n",
    "        df.astype({\"nfl_player_id_1\": \"str\"})\n",
    "        .merge(\n",
    "            tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
    "                [\"game_play\", merge_col, \"nfl_player_id\",] + use_cols\n",
    "            ],\n",
    "            left_on=[\"game_play\", merge_col, \"nfl_player_id_1\"],\n",
    "            right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .rename(columns={c: c+\"_1\" for c in use_cols})\n",
    "        .drop(\"nfl_player_id\", axis=1)\n",
    "        .merge(\n",
    "            tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
    "                [\"game_play\", merge_col, \"nfl_player_id\"] + use_cols\n",
    "            ],\n",
    "            left_on=[\"game_play\", merge_col, \"nfl_player_id_2\"],\n",
    "            right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .drop(\"nfl_player_id\", axis=1)\n",
    "        .rename(columns={c: c+\"_2\" for c in use_cols})\n",
    "        .sort_values([\"game_play\", merge_col, \"nfl_player_id_1\", \"nfl_player_id_2\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    output_cols += [c+\"_1\" for c in use_cols]\n",
    "    output_cols += [c+\"_2\" for c in use_cols]\n",
    "    \n",
    "    if (\"x_position\" in use_cols) & (\"y_position\" in use_cols):\n",
    "        index = df_combo['x_position_2'].notnull()\n",
    "        \n",
    "        distance_arr = np.full(len(index), np.nan)\n",
    "        tmp_distance_arr = np.sqrt(\n",
    "            np.square(df_combo.loc[index, \"x_position_1\"] - df_combo.loc[index, \"x_position_2\"])\n",
    "            + np.square(df_combo.loc[index, \"y_position_1\"]- df_combo.loc[index, \"y_position_2\"])\n",
    "        )\n",
    "        \n",
    "        distance_arr[index] = tmp_distance_arr\n",
    "        df_combo['distance'] = distance_arr\n",
    "        output_cols += [\"distance\"]\n",
    "        \n",
    "    df_combo['G_flug'] = (df_combo['nfl_player_id_2']==\"G\")\n",
    "    output_cols += [\"G_flug\"]\n",
    "    return df_combo, output_cols\n",
    "\n",
    "\n",
    "use_cols = [\n",
    "    'x_position', 'y_position', 'speed', 'distance',\n",
    "    'direction', 'orientation', 'acceleration', 'sa'\n",
    "]\n",
    "\n",
    "train, feature_cols = create_features(labels, train_tracking, use_cols=use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contact_id</th>\n",
       "      <th>game_play</th>\n",
       "      <th>datetime</th>\n",
       "      <th>step</th>\n",
       "      <th>nfl_player_id_1</th>\n",
       "      <th>nfl_player_id_2</th>\n",
       "      <th>contact</th>\n",
       "      <th>x_position_1</th>\n",
       "      <th>y_position_1</th>\n",
       "      <th>speed_1</th>\n",
       "      <th>...</th>\n",
       "      <th>y_position_2</th>\n",
       "      <th>speed_2</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>direction_2</th>\n",
       "      <th>orientation_2</th>\n",
       "      <th>acceleration_2</th>\n",
       "      <th>sa_2</th>\n",
       "      <th>distance</th>\n",
       "      <th>G_flug</th>\n",
       "      <th>frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58168_003392_0_37084_38567</td>\n",
       "      <td>58168_003392</td>\n",
       "      <td>2020-09-11T03:01:48.100Z</td>\n",
       "      <td>0</td>\n",
       "      <td>37084</td>\n",
       "      <td>38567</td>\n",
       "      <td>0</td>\n",
       "      <td>41.90</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>19.88</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.07</td>\n",
       "      <td>136.70</td>\n",
       "      <td>88.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.543017</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58168_003392_0_37084_G</td>\n",
       "      <td>58168_003392</td>\n",
       "      <td>2020-09-11T03:01:48.100Z</td>\n",
       "      <td>0</td>\n",
       "      <td>37084</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>41.90</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58168_003392_0_37211_46445</td>\n",
       "      <td>58168_003392</td>\n",
       "      <td>2020-09-11T03:01:48.100Z</td>\n",
       "      <td>0</td>\n",
       "      <td>37211</td>\n",
       "      <td>46445</td>\n",
       "      <td>0</td>\n",
       "      <td>39.59</td>\n",
       "      <td>17.07</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>18.08</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>148.93</td>\n",
       "      <td>92.39</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.258014</td>\n",
       "      <td>False</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58168_003392_0_37211_G</td>\n",
       "      <td>58168_003392</td>\n",
       "      <td>2020-09-11T03:01:48.100Z</td>\n",
       "      <td>0</td>\n",
       "      <td>37211</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>39.59</td>\n",
       "      <td>17.07</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58168_003392_0_38556_G</td>\n",
       "      <td>58168_003392</td>\n",
       "      <td>2020-09-11T03:01:48.100Z</td>\n",
       "      <td>0</td>\n",
       "      <td>38556</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>41.93</td>\n",
       "      <td>30.61</td>\n",
       "      <td>0.67</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135229</th>\n",
       "      <td>58582_003121_90_48220_G</td>\n",
       "      <td>58582_003121</td>\n",
       "      <td>2021-10-12T02:42:29.000Z</td>\n",
       "      <td>90</td>\n",
       "      <td>48220</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>32.92</td>\n",
       "      <td>25.29</td>\n",
       "      <td>2.52</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135230</th>\n",
       "      <td>58582_003121_90_52493_G</td>\n",
       "      <td>58582_003121</td>\n",
       "      <td>2021-10-12T02:42:29.000Z</td>\n",
       "      <td>90</td>\n",
       "      <td>52493</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>65.01</td>\n",
       "      <td>38.81</td>\n",
       "      <td>1.33</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135231</th>\n",
       "      <td>58582_003121_90_52500_G</td>\n",
       "      <td>58582_003121</td>\n",
       "      <td>2021-10-12T02:42:29.000Z</td>\n",
       "      <td>90</td>\n",
       "      <td>52500</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>58.80</td>\n",
       "      <td>40.24</td>\n",
       "      <td>1.50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135232</th>\n",
       "      <td>58582_003121_90_52609_G</td>\n",
       "      <td>58582_003121</td>\n",
       "      <td>2021-10-12T02:42:29.000Z</td>\n",
       "      <td>90</td>\n",
       "      <td>52609</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>60.47</td>\n",
       "      <td>25.96</td>\n",
       "      <td>1.33</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135233</th>\n",
       "      <td>58582_003121_90_52619_G</td>\n",
       "      <td>58582_003121</td>\n",
       "      <td>2021-10-12T02:42:29.000Z</td>\n",
       "      <td>90</td>\n",
       "      <td>52619</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>58.77</td>\n",
       "      <td>22.06</td>\n",
       "      <td>1.32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135234 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        contact_id     game_play                  datetime  \\\n",
       "0       58168_003392_0_37084_38567  58168_003392  2020-09-11T03:01:48.100Z   \n",
       "1           58168_003392_0_37084_G  58168_003392  2020-09-11T03:01:48.100Z   \n",
       "2       58168_003392_0_37211_46445  58168_003392  2020-09-11T03:01:48.100Z   \n",
       "3           58168_003392_0_37211_G  58168_003392  2020-09-11T03:01:48.100Z   \n",
       "4           58168_003392_0_38556_G  58168_003392  2020-09-11T03:01:48.100Z   \n",
       "...                            ...           ...                       ...   \n",
       "135229     58582_003121_90_48220_G  58582_003121  2021-10-12T02:42:29.000Z   \n",
       "135230     58582_003121_90_52493_G  58582_003121  2021-10-12T02:42:29.000Z   \n",
       "135231     58582_003121_90_52500_G  58582_003121  2021-10-12T02:42:29.000Z   \n",
       "135232     58582_003121_90_52609_G  58582_003121  2021-10-12T02:42:29.000Z   \n",
       "135233     58582_003121_90_52619_G  58582_003121  2021-10-12T02:42:29.000Z   \n",
       "\n",
       "        step nfl_player_id_1 nfl_player_id_2  contact  x_position_1  \\\n",
       "0          0           37084           38567        0         41.90   \n",
       "1          0           37084               G        0         41.90   \n",
       "2          0           37211           46445        0         39.59   \n",
       "3          0           37211               G        0         39.59   \n",
       "4          0           38556               G        0         41.93   \n",
       "...      ...             ...             ...      ...           ...   \n",
       "135229    90           48220               G        0         32.92   \n",
       "135230    90           52493               G        0         65.01   \n",
       "135231    90           52500               G        0         58.80   \n",
       "135232    90           52609               G        0         60.47   \n",
       "135233    90           52619               G        0         58.77   \n",
       "\n",
       "        y_position_1  speed_1  ...  y_position_2  speed_2  distance_2  \\\n",
       "0              20.08     0.54  ...         19.88     0.66        0.07   \n",
       "1              20.08     0.54  ...           NaN      NaN         NaN   \n",
       "2              17.07     0.53  ...         18.08     1.10        0.10   \n",
       "3              17.07     0.53  ...           NaN      NaN         NaN   \n",
       "4              30.61     0.67  ...           NaN      NaN         NaN   \n",
       "...              ...      ...  ...           ...      ...         ...   \n",
       "135229         25.29     2.52  ...           NaN      NaN         NaN   \n",
       "135230         38.81     1.33  ...           NaN      NaN         NaN   \n",
       "135231         40.24     1.50  ...           NaN      NaN         NaN   \n",
       "135232         25.96     1.33  ...           NaN      NaN         NaN   \n",
       "135233         22.06     1.32  ...           NaN      NaN         NaN   \n",
       "\n",
       "        direction_2  orientation_2  acceleration_2  sa_2  distance  G_flug  \\\n",
       "0            136.70          88.92            0.90  0.89  1.543017   False   \n",
       "1               NaN            NaN             NaN   NaN       NaN    True   \n",
       "2            148.93          92.39            2.03  2.03  1.258014   False   \n",
       "3               NaN            NaN             NaN   NaN       NaN    True   \n",
       "4               NaN            NaN             NaN   NaN       NaN    True   \n",
       "...             ...            ...             ...   ...       ...     ...   \n",
       "135229          NaN            NaN             NaN   NaN       NaN    True   \n",
       "135230          NaN            NaN             NaN   NaN       NaN    True   \n",
       "135231          NaN            NaN             NaN   NaN       NaN    True   \n",
       "135232          NaN            NaN             NaN   NaN       NaN    True   \n",
       "135233          NaN            NaN             NaN   NaN       NaN    True   \n",
       "\n",
       "        frame  \n",
       "0         300  \n",
       "1         300  \n",
       "2         300  \n",
       "3         300  \n",
       "4         300  \n",
       "...       ...  \n",
       "135229    840  \n",
       "135230    840  \n",
       "135231    840  \n",
       "135232    840  \n",
       "135233    840  \n",
       "\n",
       "[135234 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The first query is a simple way to filter out false positives as two players cannot be touching if they are \n",
    "#more than 2 yards away from each other (are very unlikely to be at the least)\n",
    "train_filtered = train.query('not distance>2').reset_index(drop=True)\n",
    "\n",
    "#This query is done by me in order to reduce the number of frames seen in the hope that this will lead to a greater\n",
    "#temporal understanding as the frames can be farther away, what I did not realize is that a better solution would \n",
    "#be to simply pick 24 frames that are farther away than the ones that are right next to the current frame! \n",
    "\n",
    "#I did not realize this until after seeing the winning solution\n",
    "train_filtered = train_filtered.query('step % 5 == 0').reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Control for the frames not being exactly 60 fps\n",
    "train_filtered['frame'] = (train_filtered['step']/10*59.94+5*59.94).astype('int')+1\n",
    "\n",
    "train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del train, labels, train_tracking\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Simple transforms to avoid overfitting to a particular view\n",
    "train_aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    A.Normalize(mean=[0.], std=[1.]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "#Standard transforms for image data\n",
    "valid_aug = A.Compose([\n",
    "    A.Normalize(mean=[0.], std=[1.]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 481/481 [00:14<00:00, 32.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary that can \n",
    "video2helmets = {}\n",
    "train_helmets_new = train_helmets.set_index('video')\n",
    "for video in tqdm(train_helmets.video.unique()):\n",
    "    video2helmets[video] = train_helmets_new.loc[video].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del train_helmets, train_helmets_new\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 240/240 [01:40<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "video2frames = {}\n",
    "\n",
    "for game_play in tqdm(train_video_metadata.game_play.unique()):\n",
    "    for view in ['Endzone', 'Sideline']:\n",
    "        video = game_play + f'_{view}.mp4'\n",
    "        video2frames[video] = max(list(map(lambda x:int(x.split('_')[-1].split('.')[0]), \\\n",
    "                                           glob.glob(f'kaggle/train/frames/{video}*'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I believe this dataset is best part of the provided notebook. \n",
    "#I think that I would have been able to put a model together, but the construction of the dataset and dataloader\n",
    "#are done brilliantly. They are a pairwise join of all of the players at each time frame, the window option is \n",
    "#an easy way to determine how many frames one wants to use in order to incorporate temporal data\n",
    "\n",
    "#It is currently set to 10 as I was training a resnet with 10 neighbors who are all at least 5 frames away.\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, aug=train_aug, mode='train'):\n",
    "        self.df = df\n",
    "        self.frame = df.frame.values\n",
    "        self.feature = df[feature_cols].fillna(-1).values\n",
    "        self.players = df[['nfl_player_id_1','nfl_player_id_2']].values\n",
    "        self.game_play = df.game_play.values\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # @lru_cache(1024)\n",
    "    # def read_img(self, path):\n",
    "    #     return cv2.imread(path, 0)\n",
    "   \n",
    "    def __getitem__(self, idx):   \n",
    "        window = 10\n",
    "        frame = self.frame[idx]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            frame = frame + random.randint(-6, 6)\n",
    "\n",
    "        players = []\n",
    "        for p in self.players[idx]:\n",
    "            #G represents the ground in this dataset\n",
    "            if p == 'G':\n",
    "                players.append(p)\n",
    "            else:\n",
    "                players.append(int(p))\n",
    "        \n",
    "        imgs = []\n",
    "        for view in ['Endzone', 'Sideline']:\n",
    "            video = self.game_play[idx] + f'_{view}.mp4'\n",
    "\n",
    "            tmp = video2helmets[video]\n",
    "            #below is commented out from the original notebook\n",
    "#             tmp = tmp.query('@frame-@window<=frame<=@frame+@window')\n",
    "            tmp[tmp['frame'].between(frame-window, frame+window)]\n",
    "            #below is also commented out from the original notebook\n",
    "            tmp = tmp[tmp.nfl_player_id.isin(players)]#.sort_values(['nfl_player_id', 'frame'])\n",
    "            tmp_frames = tmp.frame.values\n",
    "            tmp = tmp.groupby('frame')[['left','width','top','height']].mean()\n",
    "#0.002s\n",
    "\n",
    "            bboxes = []\n",
    "    #here we go though the window range and set the bounding boxes, then we do it with the images themselves\n",
    "    #this contructs a 3d list of images that we can feed into the neural network\n",
    "            for f in range(frame-window, frame+window+1, 1):\n",
    "                if f in tmp_frames:\n",
    "                    x, w, y, h = tmp.loc[f][['left','width','top','height']]\n",
    "                    bboxes.append([x, w, y, h])\n",
    "                else:\n",
    "                    bboxes.append([np.nan, np.nan, np.nan, np.nan])\n",
    "            bboxes = pd.DataFrame(bboxes).interpolate(limit_direction='both').values\n",
    "            bboxes = bboxes[::4]\n",
    "\n",
    "            if bboxes.sum() > 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                flag = 0\n",
    "#0.03s\n",
    "                    \n",
    "            for i, f in enumerate(range(frame-window, frame+window+1, 4)):\n",
    "                img_new = np.zeros((256, 256), dtype=np.float32)\n",
    "\n",
    "                if flag == 1 and f <= video2frames[video]:\n",
    "                    img = cv2.imread(f'kaggle/train/frames/{video}_{f:04d}.jpg', 0)\n",
    "                    #This may need to be grayscale to work\n",
    "                    #img = np.asarray(Image.fromarray(f'kaggle/train/frames/{video}_{f:04d}.jpg').convert('L'))\n",
    "                    #print(img)\n",
    "                    x, w, y, h = bboxes[i]\n",
    "                    #A crop based on the bounding box\n",
    "                    img = img[int(y+h/2)-128:int(y+h/2)+128,int(x+w/2)-128:int(x+w/2)+128].copy()\n",
    "                    img_new[:img.shape[0], :img.shape[1]] = img\n",
    "                    \n",
    "                    #plt.imshow(img_new)\n",
    "                \n",
    "                \n",
    "                imgs.append(img_new)\n",
    "                \n",
    "            \n",
    "#0.06s\n",
    "                \n",
    "        feature = np.float32(self.feature[idx])\n",
    "        #change dimensions so they line up, return features as the non-contact features in the dataset to be \n",
    "        #used for prediciton\n",
    "        img = np.array(imgs).transpose(1, 2, 0)    \n",
    "        img = self.aug(image=img)[\"image\"]\n",
    "        label = np.float32(self.df.contact.values[idx])\n",
    "        \n",
    "\n",
    "        return img, feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        #Pretrained on imagenet, uses the resnet to get 500 dimension representation of the input\n",
    "        #in_chans is dependent on the size of the window (see better code for switching models in the\n",
    "        #evals models notebook)\n",
    "        \n",
    "        #The backbone output for endzone and sideline is then combined with MLP that takes in position features\n",
    "        #of the players. Output is single number representing whether or not there is collision based on some\n",
    "        #threshold\n",
    "        self.backbone = timm.create_model(CFG['model'], pretrained=True, num_classes=500, in_chans=6)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(18, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.fc = nn.Linear(64+500*2, 1)\n",
    "\n",
    "        #Change batch size and cut number of channels in half before going through the model to fit resnet\n",
    "    def forward(self, img, feature):\n",
    "        b, c, h, w = img.shape\n",
    "        img = img.reshape(b*2, c//2, h, w)\n",
    "        img = self.backbone(img).reshape(b, -1)\n",
    "        feature = self.mlp(feature)\n",
    "        y = self.fc(torch.cat([img, feature], dim=1))\n",
    "        return y\n",
    "    \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This did not work to fix my jupyter woes when it would crash\n",
    "#import dill\n",
    "#dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Linear(in_features=512, out_features=500, bias=True)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=64, bias=True)\n",
       "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=1064, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output of model, actually a fairly simple system\n",
    "model = Model()\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this code to evaluate on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BCE loss for the output\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple loss computation, used to store the best checkpoint\n",
    "def evaluate(model, loader_val, *, compute_score=True, pbar=None):\n",
    "    \"\"\"\n",
    "    Predict and compute loss and score\n",
    "    \"\"\"\n",
    "    tb = time.time()\n",
    "    in_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    n_sum = 0\n",
    "    y_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    if pbar is not None:\n",
    "        pbar = tqdm(desc='Predict', nrows=78, total=pbar)\n",
    "        \n",
    "    total= len(loader_val)\n",
    "\n",
    "    for ibatch,(img, feature, label) in tqdm(enumerate(loader_val),total = total):\n",
    "        # img, feature, label = [x.to(device) for x in batch]\n",
    "        img = img.to(device)\n",
    "        feature = feature.to(device)\n",
    "        n = label.size(0)\n",
    "        label = label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(img, feature)\n",
    "        loss = criterion(y_pred.view(-1), label)\n",
    "\n",
    "        n_sum += n\n",
    "        loss_sum += n * loss.item()\n",
    "        \n",
    "        if pbar is not None:\n",
    "            pbar.update(len(img))\n",
    "        \n",
    "        del loss, img, label\n",
    "        gc.collect()\n",
    "\n",
    "    loss_val = loss_sum / n_sum\n",
    "\n",
    "\n",
    "    ret = {'loss': loss_val,\n",
    "           'time': time.time() - tb}\n",
    "    \n",
    "    model.train(in_training) \n",
    "    gc.collect()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test and validation loaders, saw big increase in speed when I increased num_workers.\n",
    "train_set,valid_set = train_test_split(train_filtered,test_size=0.05, random_state=42,stratify = train_filtered['contact'])\n",
    "train_set = MyDataset(train_set, train_aug, 'train')\n",
    "train_loader = DataLoader(train_set, batch_size=CFG['train_bs'], shuffle=True, num_workers=20, pin_memory=True,drop_last=True)\n",
    "valid_set = MyDataset(valid_set, valid_aug, 'test')\n",
    "valid_loader = DataLoader(valid_set, batch_size=CFG['valid_bs'], shuffle=False, num_workers=20, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6762, 26)\n",
      "                        contact_id     game_play                  datetime  \\\n",
      "83364      58418_000637_25_37161_G  58418_000637  2021-01-03T18:24:39.900Z   \n",
      "93974      58510_000152_65_43307_G  58510_000152  2021-09-12T17:10:13.800Z   \n",
      "78337  58401_002419_25_48259_52486  58401_002419  2020-12-27T19:49:37.900Z   \n",
      "15955   58202_000546_5_42445_47939  58202_000546  2020-09-27T17:20:41.500Z   \n",
      "4731       58180_000986_80_45008_G  58180_000986  2020-09-13T21:03:11.800Z   \n",
      "\n",
      "       step nfl_player_id_1 nfl_player_id_2  contact  x_position_1  \\\n",
      "83364    25           37161               G        0         38.44   \n",
      "93974    65           43307               G        0         70.89   \n",
      "78337    25           48259           52486        1         29.26   \n",
      "15955     5           42445           47939        0        108.65   \n",
      "4731     80           45008               G        0         45.56   \n",
      "\n",
      "       y_position_1  speed_1  ...  y_position_2  speed_2  distance_2  \\\n",
      "83364         30.40     1.78  ...           NaN      NaN         NaN   \n",
      "93974         24.02     1.32  ...           NaN      NaN         NaN   \n",
      "78337         19.66     6.11  ...         19.16     4.93        0.53   \n",
      "15955         26.26     1.93  ...         27.51     1.72        0.17   \n",
      "4731          44.94     5.73  ...           NaN      NaN         NaN   \n",
      "\n",
      "       direction_2  orientation_2  acceleration_2  sa_2  distance  G_flug  \\\n",
      "83364          NaN            NaN             NaN   NaN       NaN    True   \n",
      "93974          NaN            NaN             NaN   NaN       NaN    True   \n",
      "78337       147.82          84.35            1.36 -1.32  0.860233   False   \n",
      "15955       209.66         296.63            2.41  2.40  1.803469   False   \n",
      "4731           NaN            NaN             NaN   NaN       NaN    True   \n",
      "\n",
      "       frame  \n",
      "83364    450  \n",
      "93974    690  \n",
      "78337    450  \n",
      "15955    330  \n",
      "4731     780  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(valid_set.df.shape)\n",
    "print(valid_set.df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdamW, fast optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
    "nbatch = len(train_loader)\n",
    "warmup = CFG['epochs_warmup'] * nbatch\n",
    "nsteps = CFG['epochs'] * nbatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine LR worked better for me\n",
    "scheduler = CosineLRScheduler(optimizer,warmup_t=warmup, warmup_lr_init=0.0, warmup_prefix=True,t_initial=(nsteps - warmup), lr_min=1e-6) \n",
    "#scheduler = StepLRScheduler(optimizer, decay_t = 5, decay_rate=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [11:02<00:00, 24.25it/s]\n",
      "100%|███████████████████████████████████████| 1691/1691 [04:19<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 1.9964 Test Loss: 0.4009 Time: 15.44 min\n",
      "res18_1_wind_200_out_mod_frames.pytorch written\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [11:12<00:00, 23.88it/s]\n",
      "100%|███████████████████████████████████████| 1691/1691 [04:19<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.5166 Test Loss: 0.3634 Time: 31.06 min\n",
      "res18_1_wind_200_out_mod_frames.pytorch written\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [10:52<00:00, 24.61it/s]\n",
      "100%|███████████████████████████████████████| 1691/1691 [04:20<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.4187 Test Loss: 0.4833 Time: 46.37 min\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [10:52<00:00, 24.60it/s]\n",
      "100%|███████████████████████████████████████| 1691/1691 [04:23<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.4349 Test Loss: 0.2722 Time: 61.73 min\n",
      "res18_1_wind_200_out_mod_frames.pytorch written\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [10:52<00:00, 24.62it/s]\n",
      "100%|███████████████████████████████████████| 1691/1691 [04:18<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.3658 Test Loss: 0.2837 Time: 77.00 min\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 16059/16059 [10:38<00:00, 25.13it/s]\n",
      "100%|███████████████████████████████████████| 1691/1691 [04:17<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.3269 Test Loss: 0.4404 Time: 92.02 min\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████▌         | 11941/16059 [07:54<02:45, 24.94it/s]"
     ]
    }
   ],
   "source": [
    "time_val = 0.0\n",
    "tb = time.time()\n",
    "best_cv = 0\n",
    "best_loss = 1e10\n",
    "#Fairly boilerplate training loop, saving best version on val\n",
    "for iepoch in range(CFG['epochs']):\n",
    "    print('Epoch:', iepoch+1)\n",
    "    loss_sum = 0.0\n",
    "    n_sum = 0\n",
    "    total = len(train_loader)\n",
    "\n",
    "    # Train (ORIGINAL COMMENT)\n",
    "    for ibatch,(img, feature, label) in tqdm(enumerate(train_loader),total = total):\n",
    "        img = img.to(device)\n",
    "        feature = feature.to(device)\n",
    "        n = label.size(0)\n",
    "        label = label.to(device)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(img, feature).squeeze(-1)\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss_train = loss.item()\n",
    "        loss_sum += n * loss_train\n",
    "        n_sum += n\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),CFG['max_grad_norm'])\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step(iepoch * nbatch + ibatch + 1)\n",
    "        \n",
    "    val = evaluate(model, valid_loader)\n",
    "    time_val += val['time']\n",
    "    loss_train = loss_sum / n_sum\n",
    "    dt = (time.time() - tb) / 60\n",
    "    print('Epoch: %d Train Loss: %.4f Test Loss: %.4f Time: %.2f min' %\n",
    "          (iepoch + 1, loss_train, val['loss'],dt))\n",
    "    if val['loss'] < best_loss:\n",
    "        best_loss = val['loss']\n",
    "        # Save model (ORIGINAL COMMENT)\n",
    "        #I changed filenames manually, this is old filename\n",
    "        ofilename = 'res18_1_wind_200_out_mod_frames.pytorch'\n",
    "        torch.save(model.state_dict(), ofilename)\n",
    "        print(ofilename, 'written')\n",
    "    del val\n",
    "    gc.collect()\n",
    "\n",
    "dt = time.time() - tb\n",
    "print(' %.2f min total, %.2f min val' % (dt / 60, time_val / 60))\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
